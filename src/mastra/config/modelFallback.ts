import { openai } from "@ai-sdk/openai";
import { MastraLanguageModel } from "@mastra/core";
import { RuntimeContext } from "@mastra/core/di";

/**
 * Sistema de fallback de modelos seguindo padr√µes do Mastra AI
 * Usa uma hierarquia de modelos otimizada para evitar rate limits
 * Hierarquia: gpt-4o-mini ‚Üí gpt-4o ‚Üí gpt-3.5-turbo
 * PRIORIZA gpt-4o-mini para reduzir consumo de tokens e rate limits
 */

export interface ModelFallbackConfig {
  models: MastraLanguageModel[]; // Lista ordenada por prioridade
  maxRetries: number;
  agentType?: string;
}

// Estado global para controle de fallback por execu√ß√£o
const fallbackState = new Map<string, {
  currentModelIndex: number;
  attemptCount: number;
}>(); 

/**
 * Configura√ß√£o padr√£o de modelos para agentes
 * PRIORIZANDO gpt-4o-mini para evitar rate limits
 */
export const defaultModelHierarchy: ModelFallbackConfig = {
  models: [
    openai("gpt-4o-mini"), // Modelo prim√°rio - gpt-4o-mini conforme solicitado
    openai("gpt-4o"),      // Fallback 1 - compat√≠vel com streamVNext
    openai("gpt-3.5-turbo") // Fallback 2 - backup para casos extremos
  ],
  maxRetries: 1, // Reduzido para evitar ac√∫mulo de contexto
  agentType: "default"
};

/**
 * Fun√ß√£o principal para criar modelo com fallback autom√°tico
 * Implementa fallback VERDADEIRO: detecta rate limits e muda modelo
 */
export function createModelWithFallback(config: ModelFallbackConfig = defaultModelHierarchy) {
  return async ({ runtimeContext }: { runtimeContext?: RuntimeContext } = {}): Promise<MastraLanguageModel> => {
    // Gerar ID √∫nico para esta execu√ß√£o do agente
    const executionId = runtimeContext?.get('runId') || runtimeContext?.get('threadId') || 'default';
    const agentKey = `${config.agentType}_${executionId}`;
    
    // Obter ou inicializar estado do fallback para esta execu√ß√£o
    let state = fallbackState.get(agentKey);
    if (!state) {
      state = { currentModelIndex: 0, attemptCount: 0 };
      fallbackState.set(agentKey, state);
      
      // Limpar estado ap√≥s um tempo para evitar vazamento de mem√≥ria
      setTimeout(() => {
        fallbackState.delete(agentKey);
      }, 300000); // 5 minutos
    }
    
    // Selecionar o modelo atual baseado no estado
    const currentModel = config.models[state.currentModelIndex];
    const modelName = getModelName(currentModel);
    
    console.log(`üîß [MODEL FALLBACK] ${config.agentType} usando modelo: ${modelName}`);
    console.log(`üîß [MODEL FALLBACK] √çndice atual: ${state.currentModelIndex}/${config.models.length - 1}`);
    console.log(`üîß [MODEL FALLBACK] Tentativa global: ${state.attemptCount + 1}`);
    
    // üî• IMPLEMENTA√á√ÉO DO FALLBACK VERDADEIRO
    return createFallbackProxy(currentModel, config, agentKey);
  };
}

/**
 * Cria um proxy do modelo que intercepta erros de rate limit
 * e automaticamente muda para o pr√≥ximo modelo na hierarquia
 */
function createFallbackProxy(model: MastraLanguageModel, config: ModelFallbackConfig, agentKey: string): MastraLanguageModel {
  // Criar wrapper que intercepta chamadas do modelo
  const originalModel = model as any;
  
  // Proxy que intercepta m√©todos do modelo
  return new Proxy(originalModel, {
    get(target, prop, receiver) {
      const originalValue = target[prop];
      
      // Se √© uma fun√ß√£o (especialmente doGenerate, doStream)
      if (typeof originalValue === 'function') {
        return async function(...args: any[]) {
          try {
            console.log(`üöÄ [FALLBACK PROXY] Executando ${String(prop)} com modelo ${getModelName(model)}`);
            const result = await originalValue.apply(target, args);
            // ‚úÖ Sucesso: resetar contador de tentativas
            const state = fallbackState.get(agentKey);
            if (state) {
              state.attemptCount = 0;
            }
            return result;
          } catch (error: any) {
            // ‚ùå Erro: incrementar contador apenas aqui
            const state = fallbackState.get(agentKey);
            if (state) {
              state.attemptCount++;
            }
            console.log(`‚ö†Ô∏è [FALLBACK PROXY] Erro interceptado:`, error.message?.substring(0, 100));
            
            // Detectar se √© erro de rate limit
            if (isRateLimitError(error)) {
              console.log(`üö® [FALLBACK PROXY] Rate limit detectado! Tentando pr√≥ximo modelo...`);
              
              const nextModel = getNextModel(config, agentKey);
              if (nextModel) {
                console.log(`üîÑ [FALLBACK PROXY] Mudando para: ${getModelName(nextModel)}`);
                // Adicionar delay de 2 segundos antes de tentar novo modelo
                console.log(`‚è≥ [FALLBACK PROXY] Aguardando 2 segundos antes de tentar novo modelo...`);
                await new Promise(resolve => setTimeout(resolve, 2000));
                // Tentar novamente com o novo modelo
                return await (nextModel as any)[prop as keyof MastraLanguageModel](...args);
              } else {
                console.log(`‚ùå [FALLBACK PROXY] Sem mais modelos dispon√≠veis`);
                throw error;
              }
            }
            
            // Se n√£o √© rate limit, lan√ßar erro original
            throw error;
          }
        };
      }
      
      return originalValue;
    }
  });
}

/**
 * Detecta se o erro √© de rate limit
 */
function isRateLimitError(error: any): boolean {
  const errorMessage = error.message?.toLowerCase() || '';
  const errorCode = error.code?.toLowerCase() || '';
  
  return (
    errorMessage.includes('rate limit') ||
    errorMessage.includes('too many requests') ||
    errorMessage.includes('quota exceeded') ||
    errorCode === 'rate_limit_exceeded' ||
    error.status === 429
  );
}

/**
 * Obt√©m o pr√≥ximo modelo na hierarquia de fallback
 */
function getNextModel(config: ModelFallbackConfig, agentKey: string): MastraLanguageModel | null {
  const state = fallbackState.get(agentKey);
  if (!state) return null;
  
  // Se ainda temos modelos na hierarquia
  if (state.currentModelIndex < config.models.length - 1) {
    state.currentModelIndex++;
    state.attemptCount = 0; // Reset contador para novo modelo
    
    const nextModel = config.models[state.currentModelIndex];
    console.log(`‚úÖ [NEXT MODEL] Avan√ßando para modelo ${state.currentModelIndex + 1}/${config.models.length}`);
    return createFallbackProxy(nextModel, config, agentKey);
  }
  
  return null;
}

/**
 * Fun√ß√£o de debug para verificar estado atual do fallback
 */
export function getFallbackState(agentType: string, executionId: string = 'default') {
  const agentKey = `${agentType}_${executionId}`;
  const state = fallbackState.get(agentKey);
  return {
    agentKey,
    currentModelIndex: state?.currentModelIndex || 0,
    attemptCount: state?.attemptCount || 0,
    hasState: !!state
  };
}

/**
 * For√ßa avan√ßo para pr√≥ximo modelo (para testes)
 */
export function forceAdvanceModel(agentType: string, executionId: string = 'default') {
  const agentKey = `${agentType}_${executionId}`;
  const state = fallbackState.get(agentKey);
  if (state && state.currentModelIndex < defaultModelHierarchy.models.length - 1) {
    state.currentModelIndex++;
    console.log(`üîÑ [FORCE ADVANCE] For√ßando avan√ßo para modelo ${state.currentModelIndex + 1}`);
    return true;
  }
  return false;
}

/**
 * Extrai o nome do modelo para logging
 */
function getModelName(model: MastraLanguageModel): string {
  // Tentar extrair o nome do modelo - isso √© espec√≠fico do AI SDK
  try {
    return (model as any).modelId || (model as any).model || model.toString();
  } catch {
    return 'modelo-desconhecido';
  }
}

/**
 * Configura√ß√µes espec√≠ficas por tipo de agente
 * Cada agente tem uma estrat√©gia de fallback otimizada
 * Estrat√©gia: GPT-4o-mini ‚Üí GPT-4o ‚Üí GPT-3.5-turbo (priorizando menor consumo)
 */

export const strategicAgentModel = createModelWithFallback({
  models: [
    openai("gpt-4o-mini"), // Modelo prim√°rio - gpt-4o-mini conforme solicitado
    openai("gpt-4o"),      // Fallback - compat√≠vel com streamVNext
    openai("gpt-3.5-turbo") // Fallback de emerg√™ncia
  ],
  maxRetries: 1, // Reduzido para evitar ac√∫mulo de contexto
  agentType: "strategic"
});

export const operationalAgentModel = createModelWithFallback({
  models: [
    openai("gpt-4o-mini"), // Modelo prim√°rio - gpt-4o-mini conforme solicitado
    openai("gpt-4o"),      // Fallback - compat√≠vel com streamVNext
    openai("gpt-3.5-turbo") // Fallback de emerg√™ncia
  ],
  maxRetries: 1, // Reduzido para evitar ac√∫mulo de contexto
  agentType: "operational"
});

export const legalAgentModel = createModelWithFallback({
  models: [
    openai("gpt-4o-mini"), // Modelo prim√°rio - gpt-4o-mini conforme solicitado
    openai("gpt-4o"),      // Fallback - compat√≠vel com streamVNext
    openai("gpt-3.5-turbo") // Fallback de emerg√™ncia
  ],
  maxRetries: 1, // Reduzido para evitar ac√∫mulo de contexto
  agentType: "legal"
});

export const financialAgentModel = createModelWithFallback({
  models: [
    openai("gpt-4o-mini"), // Modelo prim√°rio - gpt-4o-mini conforme solicitado
    openai("gpt-4o"),      // Fallback - compat√≠vel com streamVNext
    openai("gpt-3.5-turbo") // Backup de emerg√™ncia
  ],
  maxRetries: 1, // Reduzido para evitar ac√∫mulo de contexto
  agentType: "financial"
});

/**
 * Fun√ß√£o utilit√°ria para resetar estado de fallback
 * √ötil para testes ou reset manual
 */
export function resetFallbackState(agentType?: string, executionId?: string) {
  if (agentType && executionId) {
    const agentKey = `${agentType}_${executionId}`;
    fallbackState.delete(agentKey);
    console.log(`üîÑ [MODEL FALLBACK] Estado resetado para ${agentKey}`);
  } else {
    fallbackState.clear();
    console.log(`üîÑ [MODEL FALLBACK] Todos os estados de fallback resetados`);
  }
}

/**
 * EXPLICA√á√ÉO DO FALLBACK VERDADEIRO:
 * 
 * 1. DETEC√á√ÉO: Intercepta erros de rate limit em tempo real
 * 2. FALLBACK AUTOM√ÅTICO: Muda automaticamente para pr√≥ximo modelo
 * 3. PROXY: Usa Proxy JavaScript para interceptar chamadas do modelo
 * 4. ESTADO: Mant√©m estado por agente/execu√ß√£o para evitar loops
 * 5. TRANSPAR√äNCIA: Funciona sem mudan√ßas no c√≥digo do workflow
 * 
 * Fluxo: GPT-4o (rate limit) ‚Üí GPT-4o-mini (autom√°tico) ‚Üí sucesso
 */